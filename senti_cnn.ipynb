{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import *\n",
    "from keras.layers import *\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "\n",
    "from bpemb import BPEmb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "d_emb = 300 # Embedding Dimension: 25 / 50 / 100 / 200 / 300\n",
    "vocab_size = 200000 # Vocab Size: 5000 / 10000 / 25000 / 50000 / 100000 / 200000 \n",
    "batch_size = 128\n",
    "seq_max = 200\n",
    "\n",
    "TRAIN_DIR = \"./ratings_train.txt\"\n",
    "TEST_DIR = \"./ratings_test.txt\"\n",
    "\n",
    "pre_trained = True # If True, Init Embedding Weight with Pre-Trained Vectors\n",
    "quad_layer = True # If True, Use 4 CNN Layer\n",
    "\n",
    "filter_size = 100\n",
    "\n",
    "if quad_layer:\n",
    "    windows = [2, 3, 4, 5]\n",
    "    \n",
    "else:\n",
    "    windows = [2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_trained:\n",
    "    bpemb_ko = BPEmb(lang=\"ko\", dim=d_emb, vs=vocab_size) \n",
    "else:\n",
    "    bpemb_ko = BPEmb(lang=\"ko\", vs=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (149995, 200)\n",
      "y_train shape:  (149995, 2)\n",
      "x_test shape:  (49997, 200)\n",
      "y_test shape:  (49997, 2)\n"
     ]
    }
   ],
   "source": [
    "def parsing(data):\n",
    "    first_t = data.find('\\t')\n",
    "    second_t = data[first_t + 1:].find('\\t') + first_t + 1\n",
    "    _id = data[:first_t]\n",
    "    document = data[first_t + 1:second_t]\n",
    "    label = data[second_t + 1:]\n",
    "    \n",
    "    return _id, document, label\n",
    "\n",
    "\n",
    "def add_padding(arr, max_len):\n",
    "    results = []\n",
    "    for tmp in arr:\n",
    "        for i in range(len(tmp), max_len):\n",
    "            tmp.append(0)\n",
    "        results.append(tmp)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def raw_to_data(raw):\n",
    "    x = []\n",
    "    y = []\n",
    "    for data in raw[1:]:\n",
    "\n",
    "        _, document, label = parsing(data)\n",
    "\n",
    "        tmp = bpemb_ko.encode_ids(document)\n",
    "\n",
    "        if seq_max < len(tmp):\n",
    "            continue\n",
    "\n",
    "        if len(tmp) == 0:\n",
    "            continue\n",
    "    \n",
    "        x.append(tmp)\n",
    "        y.append(label)\n",
    "        \n",
    "    return np.array(add_padding(x.copy(), seq_max)), to_categorical(np.array(y.copy(), dtype='int32'))\n",
    "\n",
    "with open(TRAIN_DIR, \"r\", encoding='utf8') as f:\n",
    "    train_raw_data = f.readlines()\n",
    "\n",
    "\n",
    "with open(TEST_DIR, \"r\", encoding='utf8') as f:\n",
    "    test_raw_data = f.readlines()\n",
    "    \n",
    "x_train, y_train = raw_to_data(train_raw_data)\n",
    "x_test, y_test = raw_to_data(test_raw_data)\n",
    "\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('x_test shape: ', x_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentalCNN:\n",
    "    def __init__(self, vocab_size, d_emb, seq_max, weight=None, trainable=False):\n",
    "\n",
    "        if weight is None:\n",
    "            self.emb_layer = Embedding(vocab_size, d_emb, input_length=seq_max)\n",
    "            \n",
    "        else:\n",
    "            self.emb_layer = Embedding(vocab_size, d_emb, input_length=seq_max, weights=[weight], trainable=trainable)\n",
    "\n",
    "            self.reshape_layer = Reshape((seq_max, d_emb, 1), input_shape=(seq_max, d_emb)) \n",
    "            \n",
    "            self.bi_cnn = Conv2D(filter_size * 1, kernel_size=(windows[0], d_emb), activation='relu', input_shape=(seq_max, d_emb, 1))\n",
    "            self.tri_cnn = Conv2D(filter_size * 1, kernel_size=(windows[1], d_emb), activation='relu', input_shape=(seq_max, d_emb, 1))\n",
    "            self.quad_cnn = Conv2D(filter_size * 1, kernel_size=(windows[2], d_emb), activation='relu', input_shape=(seq_max, d_emb, 1))\n",
    "            \n",
    "            if quad_layer:\n",
    "                self.penta_cnn = Conv2D(filter_size * 1, kernel_size=(windows[3], d_emb), activation='relu', input_shape=(seq_max, d_emb, 1))\n",
    "                \n",
    "            self.output_layer = Dense(2, activation='softmax')\n",
    "\n",
    "    def compile(self, optimizer=\"adam\"):\n",
    "        _input = Input(shape=(None, ), dtype='int32')\n",
    "\n",
    "        emb = self.emb_layer(_input)\n",
    "        reshape_emb = self.reshape_layer(emb)\n",
    "            \n",
    "        bi_res = self.bi_cnn(reshape_emb)\n",
    "        tri_res = self.tri_cnn(reshape_emb)\n",
    "        quad_res = self.quad_cnn(reshape_emb)\n",
    "            \n",
    "        bi_res = Dropout(0.3)(bi_res)\n",
    "        tri_res = Dropout(0.3)(tri_res)\n",
    "        quad_res = Dropout(0.3)(quad_res)\n",
    "                \n",
    "        a = MaxPool2D(pool_size=(seq_max - windows[0] - 1, 1))(bi_res)\n",
    "        b = MaxPool2D(pool_size=(seq_max - windows[1] - 1, 1))(tri_res)\n",
    "        c = MaxPool2D(pool_size=(seq_max - windows[2] - 1, 1))(quad_res)\n",
    "            \n",
    "        if quad_layer:\n",
    "            penta_res = self.penta_cnn(reshape_emb)\n",
    "            penta_res = Dropout(0.3)(penta_res)\n",
    "            d = MaxPool2D(pool_size=(seq_max - windows[3] - 1, 1))(penta_res)\n",
    "            result = Concatenate(axis=-1)([a, b, c, d])\n",
    "            \n",
    "        else:\n",
    "            result = Concatenate(axis=-1)([a, b, c])\n",
    "                \n",
    "        result = Flatten()(result)\n",
    "        result = self.output_layer(result)\n",
    "\n",
    "        self.model = Model(_input, result)\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "            \n",
    "    def predict(self, _input):\n",
    "        x = bpemb_ko.encode_ids(_input)\n",
    "        x = np.array(add_padding([x], seq_max))\n",
    "                    \n",
    "        res = self.model.predict(x)\n",
    "                \n",
    "        if res[0][0] < res[0][1]:\n",
    "            print(_input + \"\\n: 긍정\")\n",
    "                    \n",
    "        else:\n",
    "            print(_input + \"\\n: 부정\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 300)     60000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 200, 300, 1)  0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 199, 1, 100)  60100       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 198, 1, 100)  90100       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 197, 1, 100)  120100      reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 196, 1, 100)  150100      reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 199, 1, 100)  0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 198, 1, 100)  0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 197, 1, 100)  0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 196, 1, 100)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)    0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 100)    0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 1, 400)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 400)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            802         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 60,421,202\n",
      "Trainable params: 60,421,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\users\\msw\\appdata\\local\\continuum\\anaconda3\\envs\\moon\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 149995 samples, validate on 49997 samples\n",
      "Epoch 1/3\n",
      "149995/149995 [==============================] - 1459s 10ms/step - loss: 0.3754 - acc: 0.8332 - val_loss: 0.3365 - val_acc: 0.8580\n",
      "Epoch 2/3\n",
      "149995/149995 [==============================] - 1486s 10ms/step - loss: 0.2503 - acc: 0.8997 - val_loss: 0.3341 - val_acc: 0.8579\n",
      "Epoch 3/3\n",
      "149995/149995 [==============================] - 1487s 10ms/step - loss: 0.1643 - acc: 0.9382 - val_loss: 0.3684 - val_acc: 0.8521\n"
     ]
    }
   ],
   "source": [
    "if pre_trained:\n",
    "    sent_cnn = SentimentalCNN(\n",
    "        vocab_size=vocab_size,\n",
    "        d_emb=d_emb,\n",
    "        seq_max=seq_max,\n",
    "        weight=bpemb_ko.vectors,\n",
    "        trainable=True\n",
    "    )\n",
    "            \n",
    "else:\n",
    "    sent_cnn = SentimentalCNN(\n",
    "        vocab_size=vocab_size,\n",
    "        d_emb=d_emb,\n",
    "        seq_max=seq_max\n",
    "    )\n",
    "\n",
    "    \n",
    "sent_cnn.compile(optimizers.Adam(lr=0.001, decay=1e-6))\n",
    "sent_cnn.model.build(input_shape=(seq_max, ))\n",
    "sent_cnn.model.summary()\n",
    "hist = sent_cnn.model.fit(x_train, y_train,\n",
    "                          epochs=3,\n",
    "                          batch_size=batch_size,\n",
    "                          verbose=1,\n",
    "                          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이건 진짜 재미가 없다\n",
      ": 부정\n",
      "친구들한테싹추천함진짜대존잼\n",
      ": 긍정\n",
      "이런거 볼바에야 그냥 뽀로로 보겠다\n",
      ": 부정\n",
      "개노잼. 그냥 노잼.\n",
      ": 부정\n",
      "아이들이 넘넘 좋아하네옹^^\n",
      ": 긍정\n",
      "퍽이나 재밌다~\n",
      ": 부정\n",
      "퍽 재밌다\n",
      ": 긍정\n",
      "참 자~~알 만든 영화\n",
      ": 부정\n",
      "참 잘 만든 영화\n",
      ": 긍정\n",
      "참 자알 만든 영화\n",
      ": 부정\n"
     ]
    }
   ],
   "source": [
    "reviews = [\"이건 진짜 재미가 없다\",\n",
    "           \"친구들한테싹추천함진짜대존잼\",\n",
    "           \"이런거 볼바에야 그냥 뽀로로 보겠다\", \n",
    "           \"개노잼. 그냥 노잼.\",\n",
    "           \"아이들이 넘넘 좋아하네옹^^\",\n",
    "           \"퍽이나 재밌다~\",\n",
    "           \"퍽 재밌다\",\n",
    "           \"참 자~~알 만든 영화\",\n",
    "           \"참 잘 만든 영화\",\n",
    "           \"참 자알 만든 영화\"]\n",
    "\n",
    "for sen in reviews:\n",
    "    sent_cnn.predict(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
